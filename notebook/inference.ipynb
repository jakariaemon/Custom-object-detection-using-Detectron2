{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f217562f-7ba8-449e-9fe4-bd8d7741e332",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.data import DatasetCatalog, MetadataCatalog, build_detection_test_loader\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer, ColorMode\n",
    "from detectron2 import model_zoo\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Setup configuration\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")  # Adjust path as necessary\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 2  # Adjust based on your dataset\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # Adjust threshold as necessary\n",
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace4b64b-00bf-4f44-8af8-0ce1e326e13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpn_features = {}\n",
    "\n",
    "def extract_fpn_outputs(module, input, output):\n",
    "    for name, feature in output.items():\n",
    "        fpn_features[name] = feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dcc2d42-df0e-4e59-86a4-6f0710c8f948",
   "metadata": {},
   "outputs": [],
   "source": [
    "rpn_outputs = {}\n",
    "\n",
    "def capture_rpn_outputs(module, input, output):\n",
    "    # Capture the outputs from the RPN layer; adapt this based on the actual structure observed\n",
    "    # 'output' is expected to be a tuple where the first item is a list of Instances\n",
    "    instances = output[0][0]  # Accessing the first Instances object from the list in the first item of the tuple\n",
    "    \n",
    "    # Store the relevant outputs for later use; ensure to detach and move to CPU\n",
    "    rpn_outputs['proposal_boxes'] = instances.proposal_boxes.tensor.detach().cpu()\n",
    "    rpn_outputs['objectness_logits'] = instances.objectness_logits.detach().cpu()\n",
    "\n",
    "# Register the hook to the RPN layer; adapt 'proposal_generator' based on your model's structure\n",
    "handle = predictor.model.proposal_generator.register_forward_hook(capture_rpn_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403b4ae7-689d-4abe-9ec3-b5d96082dbcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fc023b-26e2-4112-9f7a-3dedfb70fe0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the hook\n",
    "hook_handle = predictor.model.backbone.register_forward_hook(extract_fpn_outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "196a1854-9d29-4e22-a661-f5039cef93cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an image\n",
    "image_path = 'midtest.jpg'  # Adjust path as necessary\n",
    "im = cv2.imread(image_path)\n",
    "\n",
    "# Perform inference (this will trigger the hook)\n",
    "outputs = predictor(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034b0889-fc53-45de-9002-89f7c3fba1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "# Assuming 'im' is your original image and 'top_proposal_boxes' contains the top N proposal boxes\n",
    "im_rgb = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(im_rgb)\n",
    "\n",
    "# Iterate over the proposal boxes to draw them\n",
    "for box in top_proposal_boxes:\n",
    "    x1, y1, x2, y2 = box\n",
    "    plt.gca().add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, edgecolor='r', facecolor='none', linewidth=2))\n",
    "\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fb4e3fd-d73a-4eef-a879-6017f7ade100",
   "metadata": {},
   "outputs": [
    {
     "ename": "AxisError",
     "evalue": "axis 1 is out of bounds for array of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 11\u001b[0m\n\u001b[1;32m      7\u001b[0m im_rgb \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(im, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB) \u001b[38;5;28;01mif\u001b[39;00m im\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m im\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Normalize the objectness logits to get a probability-like score between 0 and 1\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#norm = Normalize(vmin=rpn_outputs['objectness_logits'].min(), vmax=rpn_outputs['objectness_logits'].max())\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m normalized_logits \u001b[38;5;241m=\u001b[39m \u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrpn_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mobjectness_logits\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m[:, \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Create a blank heatmap with zeros and the same height and width as the original image\u001b[39;00m\n\u001b[1;32m     14\u001b[0m heatmap \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((im\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], im\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/scipy/special/_logsumexp.py:223\u001b[0m, in \u001b[0;36msoftmax\u001b[0;34m(x, axis)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Compute the softmax function.\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \n\u001b[1;32m    134\u001b[0m \u001b[38;5;124;03mThe softmax function transforms each element of a collection by\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    220\u001b[0m \n\u001b[1;32m    221\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    222\u001b[0m x \u001b[38;5;241m=\u001b[39m _asarray_validated(x, check_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 223\u001b[0m x_max \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mamax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m exp_x_shifted \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(x \u001b[38;5;241m-\u001b[39m x_max)\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m exp_x_shifted \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39msum(exp_x_shifted, axis\u001b[38;5;241m=\u001b[39maxis, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mamax\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py:2791\u001b[0m, in \u001b[0;36mamax\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2675\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_amax_dispatcher)\n\u001b[1;32m   2676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mamax\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue, initial\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue,\n\u001b[1;32m   2677\u001b[0m          where\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue):\n\u001b[1;32m   2678\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2679\u001b[0m \u001b[38;5;124;03m    Return the maximum of an array or maximum along an axis.\u001b[39;00m\n\u001b[1;32m   2680\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2789\u001b[0m \u001b[38;5;124;03m    5\u001b[39;00m\n\u001b[1;32m   2790\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2791\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaximum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2792\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py:86\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     84\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[0;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mufunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpasskwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 1"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "from scipy.special import softmax\n",
    "\n",
    "# Assuming 'im' is the original image and has been loaded already\n",
    "im_rgb = cv2.cvtColor(im, cv2.COLOR_BGR2RGB) if im.shape[2] == 3 else im\n",
    "\n",
    "# Normalize the objectness logits to get a probability-like score between 0 and 1\n",
    "#norm = Normalize(vmin=rpn_outputs['objectness_logits'].min(), vmax=rpn_outputs['objectness_logits'].max())\n",
    "normalized_logits = softmax(rpn_outputs['objectness_logits'], axis=1)[:, 1]\n",
    "\n",
    "# Create a blank heatmap with zeros and the same height and width as the original image\n",
    "heatmap = np.zeros((im.shape[0], im.shape[1]))\n",
    "\n",
    "# Assuming the proposals are in (x1, y1, x2, y2) format\n",
    "for box, score in zip(rpn_outputs['proposal_boxes'], normalized_logits):\n",
    "    x1, y1, x2, y2 = map(int, box)\n",
    "    # Fill the area within each box on the heatmap with the objectness score\n",
    "    heatmap[y1:y2, x1:x2] = np.maximum(heatmap[y1:y2, x1:x2], score)\n",
    "\n",
    "# Compute the scaling factors for width and height\n",
    "original_height, original_width = im.shape[:2]\n",
    "scale_x = original_width / heatmap.shape[1]\n",
    "scale_y = original_height / heatmap.shape[0]\n",
    "\n",
    "# Resize the heatmap to match the original image size\n",
    "resized_heatmap = cv2.resize(heatmap, (original_width, original_height), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "# Apply a colormap to the resized heatmap\n",
    "heatmap_color = cv2.applyColorMap(np.uint8(255 * resized_heatmap), cv2.COLORMAP_JET)\n",
    "\n",
    "# Overlay the colored heatmap onto the original image\n",
    "overlayed_img = cv2.addWeighted(im_rgb, 0.5, heatmap_color, 0.5, 0)\n",
    "\n",
    "# Display the original image and the one with the overlayed heatmap\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 10))\n",
    "ax[0].imshow(im_rgb)\n",
    "ax[0].set_title('Original Image')\n",
    "ax[0].axis('off')\n",
    "\n",
    "ax[1].imshow(overlayed_img)\n",
    "ax[1].set_title('RPN Objectness Heatmap Overlay')\n",
    "ax[1].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214fb52c-7394-4a8f-8db3-f4ca38758023",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib.colors import Normalize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'im' is the original image and 'rpn_outputs' contains the RPN outputs\n",
    "# Replace 'im' and 'rpn_outputs' with the actual image and RPN output variables\n",
    "\n",
    "# Normalize the objectness logits to get a probability-like score between 0 and 1\n",
    "norm = Normalize(vmin=rpn_outputs['objectness_logits'].min(), vmax=rpn_outputs['objectness_logits'].max())\n",
    "normalized_logits = norm(rpn_outputs['objectness_logits'])\n",
    "\n",
    "# Create a blank heatmap with zeros and the same height and width as the original image\n",
    "heatmap = np.zeros((im.shape[0], im.shape[1]), dtype=np.float32)\n",
    "\n",
    "# Assuming the proposals are in (x1, y1, x2, y2) format\n",
    "for box, score in zip(rpn_outputs['proposal_boxes'], normalized_logits):\n",
    "    x1, y1, x2, y2 = map(int, box)\n",
    "    # Fill the area within each box on the heatmap with the objectness score\n",
    "    heatmap[y1:y2, x1:x2] = np.maximum(heatmap[y1:y2, x1:x2], score)\n",
    "\n",
    "# Apply a colormap to the heatmap\n",
    "heatmap_color = cv2.applyColorMap(np.uint8(255 * heatmap), cv2.COLORMAP_JET)\n",
    "\n",
    "# Save the heatmap\n",
    "heatmap_path = 'rpn_objectness_heatmap.png'\n",
    "cv2.imwrite(heatmap_path, heatmap_color)\n",
    "\n",
    "# Display the heatmap\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(heatmap_color)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abd7062-3170-40f5-a254-22e4def80919",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(30, 10))  # Create a 2x3 grid\n",
    "\n",
    "# Assuming fpn_features contains 5 feature maps\n",
    "# Flatten the axes array for easier indexing\n",
    "axes_flat = axes.flatten()\n",
    "\n",
    "# Iterate through each feature map and its corresponding axis\n",
    "for i, (level, feature) in enumerate(fpn_features.items()):\n",
    "    # Plot feature map on the ith subplot\n",
    "    feature_map = feature[0, 0].detach().cpu().numpy()\n",
    "    ax = axes_flat[i]\n",
    "    ax.imshow(feature_map, cmap='viridis')\n",
    "    ax.set_title(level)\n",
    "    ax.axis('off')\n",
    "\n",
    "# If there are less feature maps than subplots, hide the unused subplots\n",
    "for j in range(i + 1, len(axes_flat)):\n",
    "    axes_flat[j].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('fpn_feature_maps_2x3_layout.png', dpi=600)  # Saving the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bce112-35d7-451b-856b-ac6a7597eb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "handle.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a14a62-47e1-478d-9225-0d8a6b29b56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "register_coco_instances(\"my_dataset_test\", {}, \"validation.json\", \"valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed283f3-384d-492b-91b8-6187ae9f91a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.data import MetadataCatalog\n",
    "\n",
    "metadata = MetadataCatalog.get(\"my_dataset_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42990e0f-db2a-463d-a5f1-efe6e3f29fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1f357d-ee9e-41a5-bd50-06cd2db7cd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "outputs = predictor(im)  # format is documented at https://detectron2.readthedocs.io/tutorials/models.html#model-output-format\n",
    "v = Visualizer(im[:, :, ::-1],\n",
    "               metadata=metadata, \n",
    "               scale=0.5, \n",
    "               instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels. This option is only available for segmentation models\n",
    ")\n",
    "out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "cv2.imwrite('output_filename.jpg', out.get_image()[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "408fc9c4-3976-450f-aec3-ec33d4456d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\n",
      "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
      "\n",
      "/home/emon/.local/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "\n",
      "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.804\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.990\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.988\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.808\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.822\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.846\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.846\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.846\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.839\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.880\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.808\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.990\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.987\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.829\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.777\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.843\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.843\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.843\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.854\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.812\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from detectron2.data import DatasetCatalog, MetadataCatalog, build_detection_test_loader\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset  \n",
    "from detectron2.data.datasets import register_coco_instances   \n",
    "from detectron2.engine import  DefaultPredictor\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.config import get_cfg \n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.utils.visualizer import ColorMode\n",
    "import cv2\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools \n",
    "\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
    "register_coco_instances(\"my_dataset_test\", {}, \"test.json\", \"test\")\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.85\n",
    "predictor = DefaultPredictor(cfg)\n",
    "evaluator = COCOEvaluator(\"my_dataset_test\", cfg, False, output_dir=\"./output/\")\n",
    "val_loader = build_detection_test_loader(cfg, \"my_dataset_test\")\n",
    "inference_on_dataset(predictor.model, val_loader, evaluator) \n",
    "\n",
    "my_dataset_test_metadata = MetadataCatalog.get(\"my_dataset_test\")\n",
    "dataset_dicts = DatasetCatalog.get(\"my_dataset_test\")\n",
    "   \n",
    "im = cv2.imread(\"midtest.jpg\")\n",
    "outputs = predictor(im)  # format is documented at https://detectron2.readthedocs.io/tutorials/models.html#model-output-format\n",
    "v = Visualizer(im[:, :, ::-1],\n",
    "               metadata=my_dataset_test_metadata, \n",
    "               scale=0.5, \n",
    "               instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels. This option is only available for segmentation models\n",
    ")\n",
    "out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "cv2.imwrite('predmidtest.jpg', out.get_image()[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30456c38-24d3-45c4-a6ad-085f3f4f5dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the visualizer with the original image\n",
    "v = Visualizer(im[:, :, ::-1], metadata=my_dataset_test_metadata, scale=0.5)\n",
    "\n",
    "# Draw only the bounding boxes on the image\n",
    "for box in outputs[\"instances\"].pred_boxes:\n",
    "    v.draw_box(box, edge_color=\"g\").to(\"cpu\")  # Draw each box in green (or any color of choice)\n",
    "\n",
    "# Save the image with bounding boxes\n",
    "cv2.imwrite('predmidtest_boxes.jpg', v.get_output().get_image()[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9a3011-7987-4790-95fd-6b06442be055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-initialize the visualizer for mask visualization\n",
    "v = Visualizer(im[:, :, ::-1], metadata=my_dataset_test_metadata, scale=0.5)\n",
    "\n",
    "# Draw only the masks on the image\n",
    "# `pred_masks` is a tensor of shape (N, H, W), where N is the number of detections\n",
    "for mask in outputs[\"instances\"].pred_masks:\n",
    "    # Convert tensor to numpy array and ensure it's boolean for `draw_binary_mask`\n",
    "    mask_array = mask.cpu().numpy()\n",
    "    v.draw_binary_mask(mask_array.astype(bool), color=\"g\")  # Draw each mask in green (or any color)\n",
    "\n",
    "# Save the image with masks\n",
    "cv2.imwrite('predmidtest_masks.jpg', v.get_output().get_image()[:, :, ::-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb9c0e4-e866-4277-af42-358f4f85c094",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = Visualizer(im[:, :, ::-1], metadata=my_dataset_test_metadata, scale=0.5)\n",
    "\n",
    "# Ensure to move the mask tensors to the CPU before converting to numpy arrays\n",
    "masks = outputs[\"instances\"].pred_masks.cpu()\n",
    "\n",
    "# Iterate through each mask and draw it on the image\n",
    "for mask in masks:\n",
    "    # Convert the tensor to a numpy array and ensure it's a boolean array\n",
    "    mask_array = mask.numpy().astype(bool)\n",
    "    v.draw_binary_mask(mask_array, color=\"g\")  # Change \"g\" to any desired color\n",
    "\n",
    "# Save the image with masks\n",
    "cv2.imwrite('predmidtest_masks.jpg', v.get_output().get_image()[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fb82ab-a876-4723-be57-11c433b2ea5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = Visualizer(im[:, :, ::-1], metadata=my_dataset_test_metadata, scale=0.5, instance_mode=ColorMode.IMAGE)\n",
    "\n",
    "# Extract the predicted boxes and draw them\n",
    "pred_boxes = outputs[\"instances\"].pred_boxes.tensor.cpu()\n",
    "\n",
    "# Iterate over each box and draw it\n",
    "for box in pred_boxes:\n",
    "    # The `draw_box` method expects a single box in (x1, y1, x2, y2) format, so no conversion is needed here\n",
    "    v.draw_box(box, edge_color=\"g\")  # You can change \"g\" to any desired color\n",
    "\n",
    "# Convert the visualized image back to BGR so it can be saved with OpenCV\n",
    "visualized_image = v.get_output().get_image()[:, :, ::-1]\n",
    "\n",
    "# Save the visualized image with bounding boxes\n",
    "cv2.imwrite('predmidtest_boxes.jpg', visualized_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f684fcbf-04c8-4385-8e15-914b1781e19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "from torchvision.transforms.functional import resize\n",
    "\n",
    "def visualize_objectness_logits(image, objectness_logits, feature_level):\n",
    "    \"\"\"\n",
    "    Visualize objectness logits on the image for a specific feature level (e.g., P2 to P6).\n",
    "    Args:\n",
    "    - image: Original image in numpy array format (HxWxC).\n",
    "    - objectness_logits: Objectness logits tensor (1xCxHxW).\n",
    "    - feature_level: The pyramid level (e.g., 'P2').\n",
    "    \"\"\"\n",
    "    # Apply sigmoid to convert logits into probabilities\n",
    "    objectness_probs = torch.sigmoid(objectness_logits).cpu().numpy()\n",
    "\n",
    "    # Select the objectness probability map for 1:1 aspect ratio anchor (assuming it's the first channel)\n",
    "    objectness_map = objectness_probs[0, 0, :, :]\n",
    "\n",
    "    # Resize objectness map to match the original image size\n",
    "    objectness_map_resized = cv2.resize(objectness_map, (image.shape[1], image.shape[0]))\n",
    "\n",
    "    # Normalize the objectness map for better visualization\n",
    "    objectness_map_normalized = cv2.normalize(objectness_map_resized, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "\n",
    "    # Convert to a colored heatmap for visualization\n",
    "    heatmap = cv2.applyColorMap(objectness_map_normalized, cv2.COLORMAP_JET)\n",
    "\n",
    "    # Overlay heatmap on the original image\n",
    "    overlayed_image = cv2.addWeighted(image, 0.6, heatmap, 0.4, 0)\n",
    "\n",
    "    return overlayed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8f9564-2f9d-4e73-b964-68f2cd654dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "rpn_outputs = {\n",
    "    \"objectness_logits\": None,\n",
    "    \"anchor_deltas\": None,\n",
    "}\n",
    "\n",
    "def capture_rpn_outputs(module, input, output):\n",
    "    # Assuming the structure of the 'output' dict and capturing the required information\n",
    "    rpn_outputs[\"objectness_logits\"] = output[\"pred_objectness_logits\"].detach()\n",
    "    rpn_outputs[\"anchor_deltas\"] = output[\"pred_anchor_deltas\"].detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c5f3ea-5a00-4741-87ce-2683312fbd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "handle = predictor.model.proposal_generator.register_forward_hook(capture_rpn_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f20ee0-bd33-4214-85d0-9f560d0cfa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_level = 'P2'\n",
    "overlayed_image = visualize_objectness_logits(im, objectness_logits_for_P2, feature_level)\n",
    "cv2.imwrite(f'overlayed_objectness_{feature_level}.jpg', overlayed_image)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
